{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2450dd1e",
   "metadata": {},
   "source": [
    "# Reinforcement Learning in bandit tasks and gridworlds\n",
    "\n",
    "Franz Wurm<br>\n",
    "<sub>Leiden University<sub><br><br>  \n",
    "    \n",
    "This is the code for the practical session in the course \"2223-S1 Cognitive Modelling: How to build a brain\" [(link to Brightspace)](https://brightspace.universiteitleiden.nl/d2l/home/188529)<br>   \n",
    "Date: 06.10.2022<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8184f326",
   "metadata": {},
   "source": [
    "**Useful references**\n",
    "- Sutton, R. S., & Barto, A. G. (2018). Reinforcement learning: An introduction. MIT press."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23253e22",
   "metadata": {},
   "source": [
    "## 1. General information\n",
    "\n",
    "### Goal of this practical\n",
    "\n",
    "In this practical, you are going to implement a reinforcement learning agent for a standard decision-making problem. Our agent will be able to learn the value of states and actions in the environment simply by observing their associated rewards and punishments. As discussed in the lecture, this resembles the principles of classical and operant conditioning. It is a efficient way of learning that leads to simple, habitual behavioral patterns. \n",
    "\n",
    "Step by step, we will discuss the different necessary components to realize such a reinforcement learning agent.\n",
    "\n",
    "### Setting up this notebook \n",
    "\n",
    "This notebook contains all necessary information for the first practical session.\n",
    "\n",
    "I recommend to download the file and save it to a separate folder. Optimally, this folder is easily accessible (e.g. on your desktop) or implemented in a preexisting folder structure (e.g. MyBachelor>CogMod>PracticalRL).\n",
    "\n",
    "I also recommend to work with copies. That means, you should not work on the original file, but rather work on copies. This makes sure, that you do not delete important information and always have a basis to go back to. Additionaly, you could implement version control, meaning you save your work to a new file from time to time (e.g., filename_v1, filename_v2). This makes sure you dont lose too much progress in case your computer shuts down or you forgot to press the save button.\n",
    "\n",
    "The exercise is constructed to be completed chronologically, i.e. from top to bottom. There will be blocks with text, such as the text that you are currently reading, as well as blocks of code, where you can execute commands and computations.\n",
    "\n",
    "The whole tutorial is written in Python3.\n",
    "\n",
    "For working on the excersises of the practical I suggest you use [Google Colab](https://colab.research.google.com/) or [Jupyter-notebook](https://jupyter.org/). While Google Colab is straight foward to use, Jupyter-notebook might require you to install additional software (e.g., [Anaconda](https://anaconda.org/))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7b1ebc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, let's import some packages that we might need\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f89d82",
   "metadata": {},
   "source": [
    "## 2. k-armed bandit task\n",
    "\n",
    "In the [bandit problem](https://en.wikipedia.org/wiki/Multi-armed_bandit), the agent is faced with a choice between multiple options. Named after slot machines in a casino, our one and two-armed versions in the lecture helped to demonstrate the two core principles that allow the agent to achieve its goal of reward maximzation. \n",
    "\n",
    "The first thing we will do is to create a _function_ that generates samples from playing such a bandit. \n",
    "\n",
    "The function below samples reward drawn from a uniform distribution based on prespecified reward probabilities. As you can see, the function takes input parameters that dictate the number of trials to sample (<code>n_trials</code>) and the reward probablities (<code>probs</code>). \n",
    "\n",
    "The function returns and array with reward samples (<code>reward_samples</code>) and the reward probabilities (<code>reward_probs</code>) that generated it. Please note, that specifying more than one reward probabilities (e.g., <code>[0.1, 0,9]</code>) will sample two independent bandits and return them as a matrix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e993fb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_uniform_bandit(n_trials,probs):\n",
    "    \n",
    "    #preallocation\n",
    "    reward_samples = np.empty((len(probs),n_trials))\n",
    "    reward_probs = np.empty((len(probs),n_trials))\n",
    "    \n",
    "    for i in np.arange(len(probs)):\n",
    "        \n",
    "        #get samples for each bandit\n",
    "        reward_samples[i,:] = np.random.binomial(1,probs[i],n_trials)\n",
    "        reward_probs[i,:] = probs[i]\n",
    "        \n",
    "    \n",
    "    return reward_samples, reward_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e752eec",
   "metadata": {},
   "source": [
    "As discussed in the lecture, the two principles of reinforcement learning are\n",
    "- the learning rule: updating of expectations based on observations \n",
    "- the decision rule: taking actions based on expectations\n",
    "\n",
    "\n",
    "## 3. Learning rule \n",
    "\n",
    "The learning rule should help our agent to build up expectations about its environment.\n",
    "\n",
    "In this section, we are going to demonstrate why a weighted-average method outperforms a standard average method in changing environments.\n",
    "\n",
    "### Improving learning... by calculating the average\n",
    "\n",
    "As a first step, let us have a look at the performance of the averaging method for incremental learning in a stable environment.\n",
    "\n",
    "We specify the learning rule for incremental averaging in acordance with the formula introduced in the lecture.\n",
    "\n",
    "$\\LARGE V_{t+1} = V_{t} + \\frac{1} {t}(R_{t} - V_{t})$,\n",
    "\n",
    ">where $V$ is the estimated average value,<br>\n",
    ">and $t$ is the current time step.<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fca64d04",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_trials = 1000\n",
    "prob = [0.7]\n",
    "\n",
    "reward_samples, reward_probs = generate_uniform_bandit(n_trials,prob)\n",
    "#a = np.random.binomial(1,0.7,10)\n",
    "\n",
    "\n",
    "mean_avg = np.empty((1,n_trials+1))\n",
    "for iT in np.arange(n_trials):\n",
    "    if iT==0:\n",
    "        mean_avg[0,0] = 0.5\n",
    "    mean_avg[0,iT+1] = mean_avg[0,iT] + (1/(iT+1))*(reward_samples[0,iT]-mean_avg[0,iT])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2df5654",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\" role=\"alert\">\n",
    "<h2> Challenge </h2><br>\n",
    "\n",
    "Show that the incremental average mean indeed converged to the true mean.\n",
    "    \n",
    ">HINT: Use the <code>print</code> function and the matplotlib methods, loaded as <code>plt</code>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9e9ad4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: print and plot results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f6807f",
   "metadata": {},
   "source": [
    "### ...in changing environments\n",
    "\n",
    "Now that we have seen that the average method of incremental learning does a good job in estimating the average on a step-by-step basis, we are going to extend this example to cover changing environments. \n",
    "\n",
    "As shown in the lecture (slide 12), the environment undergoes sudden changes in reward probabilities.\n",
    "\n",
    "We also implement the weighted-average method as introduced in the lecture.\n",
    "\n",
    "$\\LARGE V_{t+1} = V_{t} + \\alpha(R_{t} - V_{t})$,\n",
    "\n",
    ">where $\\alpha$ is the learning rate.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b3573e",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\" role=\"alert\">\n",
    "<h2> Challenge </h2><br>\n",
    "\n",
    "In the code box below, the method vor the average method is already implemented.\n",
    "\n",
    "Add the weighted average method and plot the results.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8c8ce71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Add weighted-average method\n",
    "\n",
    "n_trials = 1000\n",
    "reward_prob = [0.7, 0.3, 0.7, 0.3, 0.7, 0.3]\n",
    "#reward_prob = [1, 0, 1, 0]\n",
    "\n",
    "\n",
    "reward_samples, reward_probs = generate_uniform_bandit(n_trials,reward_prob)\n",
    "#print(reward_samples)\n",
    "\n",
    "# flattening a m-by-n matrix into a 1-by-m*n vector (simulating a changing environment)\n",
    "reward_samples = np.asarray(reward_samples)\n",
    "reward_samples = reward_samples.flatten()\n",
    "reward_probs = np.asarray(reward_probs)\n",
    "reward_probs = reward_probs.flatten()\n",
    "# fyi, this problem could also be solved by multiple loops\n",
    "\n",
    "mean_avg = np.empty((1,len(reward_prob)*n_trials+1))\n",
    "for iT in np.arange(len(reward_prob)*n_trials):    \n",
    "    if iT==0: #initialize at time point 0\n",
    "        mean_avg[0,0] = 0.5  \n",
    "    #update estimated values    \n",
    "    mean_avg[0,iT+1] = mean_avg[0,iT] + (1/(iT+1))*(reward_samples[iT]-mean_avg[0,iT])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "821cb217",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369a1fec",
   "metadata": {},
   "source": [
    "## 3. Decision rule\n",
    "\n",
    "So far, we have only dealt with prediction problems. However, the most powerful application of reinforcement learning lies in solving control problems. \n",
    "\n",
    "The decision rule (or policy) defines the agent's way of behaving at a given time or state. This is and extension of the previous prediction problem, as we now also consider actions given each state. Put differently, the policy is a mapping between states and actions and it corresponds to what psychologists sometimes call a stimulus-response association.\n",
    "\n",
    "\n",
    "### Improving decision making... by exploration\n",
    "\n",
    "In order to replicate the results for the improvement of decision-making as shown in the lecture (slide 15), we first need to set up a new bandit function.\n",
    "\n",
    "This new bandit function again takes two input parameters and give two output parameter. In contrast to the previous bandit, the current bandit will draw its reward from a normal distribution. So we will not specify reward probabilities but rather <code>reward_means</code> as the second argument.\n",
    "\n",
    "\n",
    "<div class=\"alert alert-warning\" role=\"alert\">\n",
    "<h2> Challenge </h2><br>\n",
    "\n",
    "Add a function to the routine below that does sample rewards from a normal distribution with a mean of <code>means</code> and a standard deviation of <code>1</code>.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1398f119",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_normal_bandit(n_trials,means):\n",
    "    \n",
    "    # set the same random seed so we replicate the same random values\n",
    "    #np.random.seed(2)\n",
    "    \n",
    "    #preallocation\n",
    "    reward_samples = np.empty((len(means),n_trials))\n",
    "    reward_means = np.empty((len(means),n_trials))\n",
    "    \n",
    "    for i in np.arange(len(means)):\n",
    "    \n",
    "        #get samples for each bandit\n",
    "        reward_samples[i,:] = ???\n",
    "        reward_means[i,:] = means[i]\n",
    "        \n",
    "    \n",
    "    return reward_samples, reward_means\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1deacbcc",
   "metadata": {},
   "source": [
    "During the lecture, we introduced a few different decision rules and of course the list in not exhaustive. Below you find the most commonly used methods:\n",
    "- random method\n",
    "- greedy method\n",
    "- e-greedy method\n",
    "- softmax method\n",
    "\n",
    "Let's first implement the different methods for action selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4199f99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def action_random(values):\n",
    "    nOptions = len(values)\n",
    "    actions = np.arange(nOptions)\n",
    "    \n",
    "    action = np.random.choice(actions)\n",
    "    \n",
    "    return action\n",
    "\n",
    "def action_greedy(values):\n",
    "    nOptions = len(values)\n",
    "    actions = np.arange(nOptions)\n",
    "    \n",
    "    ties = np.array(values[:,0] == np.max(values[:,0]))\n",
    "    action = np.random.choice(actions[ties]) \n",
    "    \n",
    "    return action\n",
    "\n",
    "def action_epsilon(values,epsilon):\n",
    "    nOptions = len(values)\n",
    "    actions = np.arange(nOptions)\n",
    "    \n",
    "    if (np.random.random() < epsilon):\n",
    "        action = action_random(values)\n",
    "    else:\n",
    "        action = action_greedy(values)\n",
    "    \n",
    "    return action\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147d66a4",
   "metadata": {},
   "source": [
    "Let's make a few assumptions for our simulation first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33ca46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_agents = 1000\n",
    "n_trials = 1000\n",
    "n_bandits = 10\n",
    "#reward_means = [-1.5,-1,-0.5,0,0.5,1,1.5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7850cf33",
   "metadata": {},
   "source": [
    "#### Greedy agents\n",
    "\n",
    "In the following code block I have set up a number of greedy agents.\n",
    "\n",
    "In accordance with the lecture, both the average for the rewards (<code>rewards_greedy</code>) and the percentage of optimal decisions (<code>optimum_greedy</code>) will be saved for plotting.\n",
    "\n",
    "Please note, that we implement the _average methode_ as the learning rule. As this is a stable environment, the properties of the average method guarantee that our estimates converge to the optimum/true value (at least with an appropriate action selection method...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7602050d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## greedy action selection\n",
    "\n",
    "rewards_greedy = np.empty((n_agents,n_trials))\n",
    "optimum_greedy = np.empty((n_agents,n_trials))\n",
    "for iA in np.arange(n_agents):\n",
    "\n",
    "    reward_means = np.random.normal(0,1,(n_bandits,1))\n",
    "    optimal_action = np.argmax(reward_means)\n",
    "    reward_samples, reward_probs = generate_normal_bandit(n_trials,reward_means)\n",
    "\n",
    "    #initialize the estimated values for each bandit\n",
    "    values = np.zeros((len(reward_means),1))\n",
    "    action_count = np.zeros((len(reward_means),1))\n",
    "    \n",
    "    for iT in np.arange(n_trials):    \n",
    "        #chose action\n",
    "        action = action_greedy(values)\n",
    "        action_count[action,0] =  action_count[action,0]+1\n",
    "        #calculate prediction error\n",
    "        PE = reward_samples[action,iT]-values[action,0]\n",
    "        #update value\n",
    "        values[action,0] = values[action,0] + (1/(iT+1))*PE\n",
    "        #store reward & optimal decision\n",
    "        rewards_greedy[iA,iT] = reward_samples[action,iT]\n",
    "        if action == optimal_action:\n",
    "            optimum_greedy[iA,iT] = 1\n",
    "        else:\n",
    "            optimum_greedy[iA,iT] = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8082491",
   "metadata": {},
   "source": [
    "#### $\\epsilon$-greedy action selection\n",
    "\n",
    "Basically, the $\\epsilon$-greedy agent is identical to the greedy agent with the the minor detail that on 100*$\\epsilon$% of the trials a random action will be chosen.\n",
    "\n",
    "You find the code for $\\epsilon$-greedy actions selection already above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a68e231a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "epsilon = 0.1\n",
    "\n",
    "rewards_egreedy = np.empty((n_agents,n_trials))\n",
    "optimum_egreedy = np.empty((n_agents,n_trials))\n",
    "for iA in np.arange(n_agents): \n",
    "\n",
    "    reward_means = np.random.normal(0,1,(n_bandits,1))\n",
    "    optimal_action = np.argmax(reward_means)\n",
    "    reward_samples, reward_probs = generate_normal_bandit(n_trials,reward_means)\n",
    "\n",
    "    #initialize the estimated values for each bandit\n",
    "    values = np.zeros((len(reward_means),1))\n",
    "    action_count = np.zeros((len(reward_means),1))\n",
    "    \n",
    "    for iT in np.arange(n_trials):    \n",
    "        #chose action\n",
    "        action = action_epsilon(values,epsilon)\n",
    "        action_count[action,0] =  action_count[action,0]+1\n",
    "        #calculate prediction error\n",
    "        PE = reward_samples[action,iT]-values[action,0]\n",
    "        #update value\n",
    "        values[action,0] = values[action,0] + (1/(iT+1))*PE\n",
    "        #store reward\n",
    "        rewards_egreedy[iA,iT] = reward_samples[action,iT]\n",
    "        if action == optimal_action:\n",
    "            optimum_egreedy[iA,iT] = 1\n",
    "        else:\n",
    "            optimum_egreedy[iA,iT] = 0     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b012c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 6))\n",
    "ax = fig.add_subplot(121)\n",
    "ax.plot(np.arange(n_trials),np.mean(rewards_greedy,0),'-k',label='greedy')\n",
    "ax.plot(np.arange(n_trials),np.mean(rewards_egreedy,0),'-r',label='e-greedy')\n",
    "plt.legend(loc=\"lower right\") \n",
    "ax.set_xlabel('trial')\n",
    "ax.set_ylabel('average reward')\n",
    "ax2 = fig.add_subplot(122)\n",
    "ax2.plot(np.arange(n_trials),np.mean(optimum_greedy,0),'-k',label='greedy')\n",
    "ax2.plot(np.arange(n_trials),np.mean(optimum_egreedy,0),'-r',label='e-greedy')\n",
    "plt.legend(loc=\"lower right\")   \n",
    "ax2.set_xlabel('trial')\n",
    "ax2.set_ylabel('% optimal choice')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389943db",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\" role=\"alert\">\n",
    "<h2> Challenge </h2><br>\n",
    "\n",
    "It seems that something is wrong with the code. Can you figure out why the results of our code do not replicate the simulations results of Sutton & Barto? In silico, the $\\epsilon$-greedy method should outperform the greedy method for this environment. However it does not.\n",
    "    \n",
    ">HINT: Have a look at the updating function.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75905f1d",
   "metadata": {},
   "source": [
    "### Softmax action selection\n",
    "\n",
    "Let's have a closer look at the softmax rule, as this is maybe the most widely used decision rule in the neuroscientific literature.\n",
    "\n",
    "$\\LARGE p(a)= \\frac{e ^{(\\beta * Q(a))}} {\\sum \\limits _{a'} e ^{(\\beta * Q(a'))}}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "79a16300",
   "metadata": {},
   "outputs": [],
   "source": [
    "def action_softm(values,beta):\n",
    "    nOptions = len(values)\n",
    "    actions = np.arange(nOptions)\n",
    "    \n",
    "    prob = ???\n",
    "    action = np.random.choice(actions,size = 1, p = prob) \n",
    "    \n",
    "    return action, prob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f015c76",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\" role=\"alert\">\n",
    "<h2> Challenge </h2><br>\n",
    "\n",
    "The central part of the softmax algorithm is missing.\n",
    "    \n",
    ">HINT: Use prespecified numpy methods.\n",
    "</div>\n",
    "\n",
    "If we want to put this softmax function to use, we can make a few hypothetical assumptions for our bandit task.\n",
    "\n",
    "For example, let's assume our agent has played the slot machine twice, each arm one time. The left arm lead to a reward (1), whereas the right arm did not result in a reward (0). We can translate this experience into simplified expectations (values) for the next game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c81a8dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chosen action: 1\n",
      "Action probablities: [0.73105858 0.26894142]\n"
     ]
    }
   ],
   "source": [
    "dummy_values = np.array([1,0])\n",
    "beta = 1\n",
    "\n",
    "action,prob = action_softm(dummy_values,beta)\n",
    "\n",
    "print('Chosen action: %d' % (action))\n",
    "print('Action probablities: %s' % (np.array2string(prob)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc554ab5",
   "metadata": {},
   "source": [
    "We can see, that the dummy expectations translate into action probabilites in a straightforward way. The highest value has the highest action probability.\n",
    "\n",
    "<div class=\"alert alert-warning\" role=\"alert\">\n",
    "<h2> Idea </h2><br>\n",
    "Play around with the beta value and observe the changes!\n",
    "    \n",
    "</div>\n",
    "\n",
    "As already discussed in the lecture, the beta (or 'inverse temperature') affects the so-called [gain](https://en.wikipedia.org/wiki/Gain_(electronics)). The higher the gain, the more pronounced the differences in action values get translated into action probabilities.\n",
    "\n",
    "We can plot this in a systematic way.\n",
    "\n",
    "<div class=\"alert alert-warning\" role=\"alert\">\n",
    "<h2> Challenge </h2><br>\n",
    "Setup a plot similar to slide 17 in the lecture.<br>\n",
    "- x-axis: differences in values<br>\n",
    "- y-axis: action probablity for option 1<br>\n",
    "    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5982f234",
   "metadata": {},
   "outputs": [],
   "source": [
    "betas = [0,1,2,5,10]\n",
    "x = np.arange(-2,2,0.1)\n",
    "\n",
    "all_probs = np.empty((len(betas),len(x)))\n",
    "\n",
    "## TODO: calculate action probabilites for each [beta,x] pair\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3b84c752",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## TODO: plot the results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dccac088",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "<h2> Ideas for assignments </h2><br>\n",
    "There is a wide variety of tutorials and excercised already published on Github.\n",
    "For example, an extensive introduction to model-free vs. model-based RL in a 2-step Markov decision task can be found via the link provided below. They have instresting extra assignments.\n",
    "    \n",
    "</div>\n",
    "\n",
    "[link to mf/mb RL](https://github.com/ClaireSmid/Model-free_Model-based_Workshop)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81a808c",
   "metadata": {},
   "source": [
    "## The grid world problem\n",
    "\n",
    "*Grid worlds* are simplified respresentations of the environment, that are often used for navigation simulation. As is implied by the name, grid worlds break down the environment into a grid, similar to a chess board. For the scope of this course we are going to stick with a very basic 4x4 gridworld.\n",
    "\n",
    "Before we dive further into the code, a bit more background information on gridworlds. \n",
    "\n",
    "Gridworlds are so-called [Markov Decision Processes](https://en.wikipedia.org/wiki/Markov_decision_process). In contrast to the previous bandit task, gridworlds usually are multi-step problems, meaning that actions executed in one state cannot only result in reward, but also affect the upcoming state. This necessitates the agent to not only consider the immediate reward but also the expected cumulative reward. \n",
    "\n",
    "```\n",
    "  ____    ____     ____   ____\n",
    "  ____    ____     ____   ____\n",
    "\n",
    "|| s00  |  s01  |  s02 | s03 ||\n",
    "  ____    ____     ____   ____\n",
    "    \n",
    "|| s04  |  s05  |  s06 | s07 ||\n",
    "  ____    ____     ____   ____\n",
    "  \n",
    "|| s08  |  s09  |  s10 | s11 ||\n",
    "  ____    ____     ____   ____\n",
    "   \n",
    "|| s12  |  s13  |  s14 | s15 ||\n",
    "  ____    ____     ____   ____\n",
    "  ____    ____     ____   ____\n",
    "```\n",
    "\n",
    "Our agent always starts in the same start state (s_0, top left of board). From there, it will take *steps*, that gradually move him across the board. Movement is restricted to the cardinal directions (up, down, right, left). Reward is located in the terminal state (s_15, bottom right of board). Upon arrival at the site of reward, the agent receives the reward (associated with a positive value) and will be returned to the initial state, so the whole procedure can start again. The (time)steps between start and terminal state are regarded as a *run* (or *episode*). Start state, reward and terminal state do NOT change between runs. Thus, our gridworld environment is stable. Also, because each action transitions to a new state with 100% chance, the environment is also considered deterministic.\n",
    "\n",
    "For this grid world example, we will implement the Q learning rule, which is defined as\n",
    "\n",
    "$\\LARGE Q_{new}(s,a) = Q_{old}(s,a) + \\alpha * (R + \\gamma max_{a} Q_{old}(s',a) - Q_{old}(s,a))$,\n",
    "\n",
    "    where $\\alpha$ is the learning rate,\n",
    "    $\\gamma$ is the discounting factor,\n",
    "    and s' is the next state\n",
    "\n",
    "Below, we define the most basic details for our gridworld."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d4bd9cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "nTrial = 50 #specify how many times you want to run the q-learning function\n",
    "nAgent = 1\n",
    "\n",
    "method = 'egreedy' #options: greedy | egreedy | softmax\n",
    "\n",
    "alpha = 0.1 #learning rate\n",
    "beta = 1 #inverse temperature\n",
    "gamma = 1 # discounting factor\n",
    "epsilon = 0.1 #parameter for ε-greedy action selection\n",
    "\n",
    "actions = np.array(['up', 'left', 'down', 'right'])\n",
    "\n",
    "s_0 = 0 #start state\n",
    "s_terminal = 15 #final state (goal)\n",
    "\n",
    "envsize = 4 #size of the environment (n x n)\n",
    "iZ = 0\n",
    "states= np.empty([envsize,envsize],dtype=float)\n",
    "for iY in range(envsize):\n",
    "    for iX in range(envsize):\n",
    "        states[iY][iX] = iZ\n",
    "        iZ +=1\n",
    "#print('states')\n",
    "#print(states)        \n",
    "\n",
    "movecounter = np.empty([nTrial,nAgent] )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6356d4dc",
   "metadata": {},
   "source": [
    "### Helper function\n",
    "\n",
    "Some computational steps have to be performed multiple times.\n",
    "Here we define those function so we can call them later on in the script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7971d9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is not an essential package, but it helps us to save variables for visualisation\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c319b93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def move1step(instate,action):\n",
    "    \n",
    "    coords = list(np.where(states == instate))\n",
    "    \n",
    "    if (action == 'up'):\n",
    "        coords[0] -= 1\n",
    "    if (action == 'down'):\n",
    "        coords[0] += 1\n",
    "    if (action == 'left'):\n",
    "        coords[1] -= 1\n",
    "    if (action == 'right'):\n",
    "        coords[1] += 1\n",
    "        \n",
    "    if (coords[0] < 0):\n",
    "        coords[0] = 0\n",
    "    if (coords[1] < 0):\n",
    "        coords[1] = 0 \n",
    "    if (coords[0] >= envsize):\n",
    "        coords[0] = envsize-1    \n",
    "    if (coords[1] >= envsize):\n",
    "        coords[1] = envsize-1\n",
    "        \n",
    "    outstate = states[coords[0],coords[1]]   \n",
    "    return(outstate)  \n",
    "\n",
    "def getReward(state1,state2):\n",
    "    if (state2 == s_terminal):\n",
    "        reward = 10 #final reward at the end of the run\n",
    "    else:\n",
    "        reward = 0 #intermediate reward (after each action) \n",
    "    if (state1 == state2):\n",
    "        reward = 0 #reward for running into a wall?\n",
    "    return reward    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f6c55b",
   "metadata": {},
   "source": [
    "Now that we have defined the most crucial parts for our grid world, we can put everything together.\n",
    "\n",
    "For convenience, I have already added multiple agents, as this will allow us to draw more precise conclusions from the simulation data.\n",
    "\n",
    "For the agent's parameters I have implemented very basic values. You can have a look how changing those parameters affects the performance in the grid world."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "83aa3093",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent\n",
      "0 "
     ]
    }
   ],
   "source": [
    "print('Agent')\n",
    "for iA in range(nAgent):\n",
    "    \n",
    "    print(iA, end=' ')\n",
    "    Q = np.zeros([len(actions),envsize,envsize] )\n",
    "\n",
    "    #print('initial Q matrix:\\n' + str(Q) + '\\n')\n",
    "    Q0 = deepcopy(Q)\n",
    "\n",
    "    for iT in range(nTrial): # loop for the different runs\n",
    "        #if (iT % 100) == 0:\n",
    "            #print(iT)\n",
    "        state = s_0\n",
    "        moves = 0\n",
    "        while (state != s_terminal): # loop within one run\n",
    "        \n",
    "            coord0 = np.where(states == state)\n",
    "\n",
    "            qvals = Q[:,coord0[0],coord0[1]]\n",
    "              \n",
    "            # select action using choice rules\n",
    "            if (method == 'softmax'):\n",
    "                pvals = np.exp(beta*qvals)/np.sum(np.exp(beta*qvals))\n",
    "                action = np.random.choice(actions,size = 1, p = pvals.flatten())            \n",
    "            elif (method == 'greedy'):\n",
    "                choices = np.array(qvals == np.max(qvals))\n",
    "                action = np.random.choice(actions[choices.flatten()])            \n",
    "            elif (method == 'egreedy'):\n",
    "                if (np.random.random() < epsilon):\n",
    "                    action = np.random.choice(actions, size = 1)\n",
    "                else:\n",
    "                    choices = np.array(qvals == np.max(qvals))\n",
    "                    action = np.random.choice(actions[choices.flatten()])\n",
    "            else:\n",
    "                action = np.random.choice(actions)\n",
    "        \n",
    "            # interact with environment\n",
    "            next_state = move1step(state,action)\n",
    "            #print('next state: '+str(next_state))\n",
    "            reward = getReward(state,next_state)\n",
    "        \n",
    "            actionidx = (action==actions)\n",
    "            coord1 = np.where(states == next_state) \n",
    "        \n",
    "            # update expectations using learing rules\n",
    "            Q[actionidx,coord0[0],coord0[1]] = Q[actionidx,coord0[0],coord0[1]] + alpha * (reward + gamma * np.max(Q[:,coord1[0],coord1[1]]) - Q[actionidx,coord0[0],coord0[1]])\n",
    "        \n",
    "            if ((iT == 0) and (next_state == s_terminal)):\n",
    "                #print('Q matrix before first successful run:\\n' + str(Q) + '\\n')\n",
    "                Q1 = deepcopy(Q)\n",
    "        \n",
    "            # update variables\n",
    "            state = next_state\n",
    "            moves += 1\n",
    "        \n",
    "        #end of while loop\n",
    "        movecounter[iT,iA] = moves\n",
    "    #end of trial for loop\n",
    "    #print('final Q matrix:\\n' + str(Q) + '\\n')\n",
    "    Q2 = deepcopy(Q)\n",
    "#end of agent for loop\n",
    "#print('\\n\\nFinal Q(s,a) for agent '+str(iA))\n",
    "#print(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e0b985",
   "metadata": {},
   "source": [
    "In order to better understand the agents' behavior in the gridworld we can again make use of the `print()` and `plot()` function.\n",
    "\n",
    "<div class=\"alert alert-warning\" role=\"alert\">\n",
    "<h2> Questions </h2><br>\n",
    "Before we continue, think about the performance of our agent. Do you have any hypothesis about this? What would you expect from a RL agent? How should its behavior change?\n",
    "    \n",
    "</div>\n",
    "\n",
    "In the following sections we will try to answer those questions.\n",
    "\n",
    "### Plotting performance\n",
    "\n",
    "**Question:** What is our hypothesis for the performance of our agent?\n",
    "\n",
    "**Answer:** Performance for learning agents should improve over time. Therefore we should expect to find improvements in performance for our grid world agent. Performance can be measured in multiple different ways. For our grid world example, we could define performance as is already plotted below: the number of steps it takes the agents to get from start state to terminal state. While the agent is aimlessly roaming the gridworld in the beginning, it quickly picks up the optimal path to the goal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf4ccbc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()  # Create a figure containing a single axes.\n",
    "\n",
    "xdata = range(nTrial)\n",
    "ydata = np.mean(movecounter,axis=1)\n",
    "\n",
    "ax.plot(xdata, ydata, label = method)  # Plot some data on the axes.\n",
    "\n",
    "ax.set_title('Performance over time')\n",
    "ax.set_xlabel('Run')\n",
    "ax.set_ylabel('Number of moves')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ebcf1c",
   "metadata": {},
   "source": [
    "### Plotting values and policies\n",
    "\n",
    "We can also visualize the state and action values of our gridworld. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "60b1a414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 0.]]\n",
      "[[3.69723707e+00 6.67910800e-03 4.62342661e-01 0.00000000e+00]\n",
      " [5.56907816e+00 7.40928846e+00 8.79672340e+00 9.63128631e+00]\n",
      " [0.00000000e+00 4.68559000e-02 2.70472423e+00 9.94846225e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "Qmat0 = np.max(Q0,axis=0)\n",
    "Qmat1 = np.max(Q1,axis=0)\n",
    "Qmat2 = np.max(Q2,axis=0)\n",
    "\n",
    "print(Qmat0)\n",
    "print(Qmat1)\n",
    "print(Qmat2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79fc9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3)\n",
    "\n",
    "layout = [['s0', 's5/6'], ['s1', 's4'], ['s2', 's3']]\n",
    "\n",
    "min_val, max_val = -1, 10\n",
    "\n",
    "for i in range(envsize):\n",
    "    for j in range(envsize):\n",
    "        ax1.text(i, j, str(states[j][i]), va='center', ha='center',size=10)\n",
    "        ax2.text(i, j, str(states[j][i]), va='center', ha='center',size=10)\n",
    "        ax3.text(i, j, str(states[j][i]), va='center', ha='center',size=10)\n",
    "ax1.matshow(Qmat0, cmap=plt.cm.Blues)  \n",
    "ax2.matshow(Qmat1, cmap=plt.cm.Blues)\n",
    "ax3.matshow(Qmat2, cmap=plt.cm.Blues)\n",
    "\n",
    "ax1.axes.xaxis.set_visible(False)\n",
    "ax1.axes.yaxis.set_visible(False)\n",
    "ax2.axes.xaxis.set_visible(False)\n",
    "ax2.axes.yaxis.set_visible(False)\n",
    "ax3.axes.xaxis.set_visible(False)\n",
    "ax3.axes.yaxis.set_visible(False)\n",
    "\n",
    "ax1.title.set_text('Run 0')\n",
    "ax1.title.set_size(20)\n",
    "ax2.title.set_text('Run 1')\n",
    "ax2.title.set_size(20)\n",
    "ax3.title.set_text('Run X')\n",
    "ax3.title.set_size(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db50c72c",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "<h2> Ideas for assignments </h2><br>\n",
    "The current grid world is perfectly deterministic. Actions reliably lead to the same outcome. Reward is always hidden in the same location. Recall that for our bandit example reward delivery was probabilistic. Can you implement some randomness in the grid world as well? Think about a few possible options. How could you implement those and what would be the consequences for agents' learning and decision-making?<br>\n",
    "    \n",
    ">Discuss your ideas with the workgroup teacher!\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e952341",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "<h2> Ideas for assignments </h2><br>\n",
    "So far, we have only implemented different decision rules at this point. But we could also think about different learning rules. As discussed in the lecture, a prominent example from Sutton & Barto (2018, p.132) is the cliff world. In their variant of the grid world, learning for the different learning rules translates into very distinct behavioral patterns. It nicely illustrates how small changes in the algorithm can have quite strong effects on decision making. Have a look at the example. Can you construct a cliff world and replicate the findings reported by Sutton & Barto? <br> \n",
    "    \n",
    ">Discuss your ideas with the workgroup teacher!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ac7954",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
